# Try Visual language encoders 

1. Check CLIP model (Colab):

    [CLIP model (Colab)](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb) based on [openCLIP](https://github.com/mlfoundations/open_clip?tab=readme-ov-file). 
2. Check CLIPSeg model (Colab):
    [CLIPSeg model](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb) for zero-shot image segmentation with CLIPSeg. 

3. Check Florence model (Colab):
    [Florence-2-large (Colab)](https://colab.research.google.com/#fileId=https%3A//huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb) for inference and visualization of Florence-2-large



Return [README.md](../README.md)