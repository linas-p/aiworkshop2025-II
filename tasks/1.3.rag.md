# Set up RAG system for local information search

1. Prerequisites:
    * Install packages:
        ```bash
        pip install "praisonaiagents[knowledge]" ollama streamlit chonkie tiktoken
        ```
    * Download the lightweight LLM model of text embedding:
        ```bash
        ollama pull nomic-embed-text
        ```
2. Set up the local (Ollama) usage of LLM's:
```
export OPENAI_BASE_URL=http://localhost:11434/v1
export OPENAI_API_KEY=fake-key
```

3. Test the LLM model:
```Python
from praisonaiagents import Agent

agent = Agent(instructions="You are helpful Assisant", llm="gemma3:latest")

agent.start("Why sky is Blue?")
```

4. Let's create some new local knowledge ex. file.txt.

5. Run the RAG system:
```Python
from praisonaiagents import Agent

config = {
    "vector_store": {
        "provider": "chroma",
        "config": {
            "collection_name": "praison",
            "path": ".praison"
        }
    },
    "llm": {
        "provider": "ollama",
        "config": {
            "model": "gemma3:latest",
            "temperature": 0,
            "max_tokens": 8000,
            "ollama_base_url": "http://localhost:11434",
        },
    },
    "embedder": {
        "provider": "ollama",
        "config": {
            "model": "nomic-embed-text:latest",
            "ollama_base_url": "http://localhost:11434",
            "embedding_dims": 1536
        },
    },
}

agent = Agent(
    name="Knowledge Agent",
    instructions="You answer questions based on the provided knowledge.",
    knowledge=["file.txt"], # Indexing
    knowledge_config=config,
    user_id="user1",
    llm="gemma3:latest"
)

agent.start("What is in the file.txt?") # Retrieval
```


5. Test the model via UI:
```Python
import streamlit as st
from praisonaiagents import Agent

def init_agent():
    config = {
        "vector_store": {
            "provider": "chroma",
            "config": {
                "collection_name": "praison",
                "path": ".praison"
            }
        },
        "llm": {
            "provider": "ollama",
            "config": {
                "model": "gemma3:latest",
                "temperature": 0,
                "max_tokens": 8000,
                "ollama_base_url": "http://localhost:11434",
            },
        },
        "embedder": {
            "provider": "ollama",
            "config": {
                "model": "nomic-embed-text:latest",
                "ollama_base_url": "http://localhost:11434",
                "embedding_dims": 1536
            },
        },
    }
    
    return Agent(
        name="Knowledge Agent",
        instructions="You answer questions based on the provided knowledge.",
        knowledge=["kag-research-paper.pdf"],
        knowledge_config=config,
        user_id="user1",
        llm="gemma3:latest"
    )

st.title("Knowledge Agent Chat")

if "agent" not in st.session_state:
    st.session_state.agent = init_agent()
    st.session_state.messages = []

if "messages" in st.session_state:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

prompt = st.chat_input("Ask a question...")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response = st.session_state.agent.start(prompt)
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response}) 
```

6. Run the UI and test it out!

The indepth instructions could be found in the [Praison.ai](https://docs.praison.ai/models/deepseek) documentation. 

Return [Readme](../README.md)
