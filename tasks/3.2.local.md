# Try local VLM models on Ollama

1. Check if Ollama is running:
    ```bash
    curl http://localhost:11434
    ```
2. Run the Python script `1.3.1.image_processing.py` with the following content:
    ```python
    import ollama

    response = ollama.chat(
        model='gemma3:12b',
        messages=[{
            'role': 'user',
            'content': 'Kas matoma paveikslÄ—lyje?',
            'images': ['image.png']
        }]
    )

    print(response)
    ```
3. Run the script:
    ```bash
    python3 ./1.3.1.image_processing.py
    ```


## Let's try to run video processing with Ollama
1. Check if Ollama is running:
    ```bash
    curl http://localhost:11434
    ```
2. Run the Python script `1.3.2.video_processing.py`. Produce input video `input.mp4`.

Return [README.md](../README.md)